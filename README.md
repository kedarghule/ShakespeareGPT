# deep_learning_diy
The nanoGPT folder contains an implementation of GPT completly from scratch using just numpy & torch.  
1. The first use case explored here is language generation. Specifically, the tiny Shakespeare dataset. Ablation studies are carried out to determine the relative importance of different components of the Transformer architecture using the perfromance of the model.  
1. Text Generation using this model is compared against ChatGPT & GPT-2. The GPT-2 model is fine tuned first on the Shakespeare dataset.  
1. Future work will attempt to use the same architecture to train the decoder-only language model to perform addition.
