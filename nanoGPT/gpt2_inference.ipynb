{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6e7e84-ef28-45a3-b066-1c9c4d3b00f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a98dd38-368b-4c57-b7fd-797a9b655a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghule.k/.conda/envs/llm_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad34272-c401-4426-a764-5e3951564485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "config = GPT2Config.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "224a8c20-ed9a-4cf0-957b-4c2ba09dad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghule.k/.conda/envs/llm_env/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset\n",
    "train_path = \"input.txt\"  # Path to your training text file\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=train_path,\n",
    "    block_size=128\n",
    ")\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02bb1645-e94a-45ac-9200-8c154a823701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_gpt2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31b9b5cb-a653-40a7-b739-3f988a0a749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghule.k/.conda/envs/llm_env/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='660' max='660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [660/660 01:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.735600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=660, training_loss=3.6942749948212597, metrics={'train_runtime': 108.8204, 'train_samples_per_second': 24.26, 'train_steps_per_second': 6.065, 'total_flos': 172452741120000.0, 'train_loss': 3.6942749948212597, 'epoch': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "136a8f4c-b4ec-42bb-a633-74bdfb525668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output_gpt2/tokenizer_config.json',\n",
       " 'output_gpt2/special_tokens_map.json',\n",
       " 'output_gpt2/vocab.json',\n",
       " 'output_gpt2/merges.txt',\n",
       " 'output_gpt2/added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "output_path = \"output_gpt2\"\n",
    "model.save_pretrained(output_path)\n",
    "tokenizer.save_pretrained(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e354b0c-f7e2-46cc-803e-b8d9e1e4f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6cec68b-10dd-429d-8f73-d30c2a8d762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"output_gpt2\"  # The directory where the trained model was saved\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(output_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(output_path)\n",
    "\n",
    "# Prepare the input text\n",
    "input_text = \"Thou art to blame thyself for thy follies\"  # You can use any text you'd like as a starting point\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fea46a9d-7c2e-4e09-939b-5bc2a03789f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 11, but `max_length` is set to 10. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids, max_length=10, num_return_sequences=1, no_repeat_ngram_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c018080d-3729-40f6-b69e-83ee2cfd3b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in epoch 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(\"in epoch {}\".format(i))\n",
    "    # Generate text using the model\n",
    "    output = model.generate(input_ids, max_length=1024, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    text_file = open(\"gpt2_sample.txt\", \"w\")\n",
    "    n = text_file.write(generated_text)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274b4df3-fc7d-46b4-a586-a1960eb509bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbd8005a-4a05-4688-b3a3-72f14b8001e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"gpt2_sample.txt\", \"w\")\n",
    "n = text_file.write(generated_text)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd447d89-520c-48ed-bd1c-e51a19571fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thou art to blame thyself for thy follies.\\n\\nKING RICHARD II:\\nI am not to be blamed, but to have my revenge. I\\nwill not be revenged, nor will I be accused. But\\nif thou wilt, I will not. Thou art not my fault,\\nbut my own. If thou art myself, thou shalt not blame me. For\\nit is not mine fault that thou hast done this. O, my lord, what\\nis thy fault?\\nThy fault is mine, and thou wast not thy own;\\nAnd thou didst not do this, for thou was not\\nyour own, as thou were not your own: thou\\ndidst do it, because thou hadst done it. Now, then, let me\\nspeak to thee, that I may not speak to thy\\nself. What is thy name? thou dost not know, sir? I am\\na man of the world, a man that is a king. A man\\nthat is my father, an heir to my throne. My\\nfather is the king of France, the son of a prince. He\\nwas a nobleman, who was a good man, whose\\nowns were noble, noble and noble. And, if thou know not, he\\nhad a son, which is his own son. This is\\nmy father's name, this is your name. Come, come, go, to the\\nhouse of my mother, where thou mayst see my son: I'll\\ntell thee what thou dost, or else thou willst be a fool. Go, tell me, how\\ndo you know my name and what I do? What do you\\nknow? what do I know? and, by my word, why do thou speak\\nto me? why, do not thou say, 'I know' not? 'Why, not'?'\\nO, good lord! thou must know me not; thou cannot know\\nwhat thou meanest, when thou havest spoken to me; and I, in\\nthe name of thy father and my brother, have\\nnot spoken. 'O my God, is this my daughter?'\\n'Ay, no, it is. Nay, she is, her name is 'Nurse.'\\nWhat, are you not a nurse? O my Lord, nurse, you are a\\nman of this world. You are my nurse's daughter, your\\nwife's wife, thy wife's husband, his wife. Your\\nname is nurse; your wife is her husband; thy husband is yours; his\\nhusband is hers; yours is theirs; theirs is ours; mine is none; my\\nother is nothing; I have no name; no\\nkind of name: my wife and mine are not names; they are\\nnothing; their names are nothing. They are all names, all\\nnames, names: they all are names. All names\\nare names of mine; all of them are nameless; none of\\nthem are named; the rest are none. The\\nmost names in this life are mine. There is no other name\\nin this time; there is only one name in the whole world; it\\ndoes not mean'my name,' 'My name.' 'No,' no; 'no,'\\nno;'no, there's no.' 'Yes,' yes; but 'not,' not 'yes.''No, here's none, none: 'there's\\nnone,' none:' 'none:'\\nthere is one; one, one: there are no names but one. Here's one\\nand there, two: one and there; two, three: three, four: four, five: five, six: six, seven: seven, eight: nine, ten: ten, eleven: twelve: thirteen: fourteen: fifteen: sixteen: seventeen: eighteen: nineteen: twenty: thirty: forty: fifty: sixty: seventy: eighty: ninety: nay, nays, ne'er, say 'twere not: and\\ntwenty, twenty, thirty, forty, fifty, sixty, seventy, eighty, ninety, twere none.' What, so many? so few? how many, such a number?--\\nWhy? for I cannot tell. Why, now, poor man!\\nHow many are there? twenty-five, fifteen, sixteen, seventeen, eighteen, nineteen, thirtieth,twentieth:--twelve, twelve, fourteen,--thirt,thirty, thousand:twixt, hundred, of all, nine hundred and twenty. How many\\nhave you? ten thousand, more, than thou canst tell;--nine, like a thousand; ten hundred thousand. Where are these? where are they? Where is they, O poor\\nwoman? in thy house? there they lie, with their heads in their hands, their\\nsides in a bed, they have their arms in\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98591565-c602-4e60-828f-a03c5bdb418f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9216"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3c15331-8170-4f8e-9179-4c3b484bad22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:00<00:00, 473kB/s]\n",
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 510M/510M [00:09<00:00, 55.4MB/s] \n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255/255 [00:00<00:00, 142kB/s]\n",
      "Downloading (â€¦)olve/main/vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 798k/798k [00:00<00:00, 61.7MB/s]\n",
      "Downloading (â€¦)olve/main/merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 37.8MB/s]\n",
      "Downloading (â€¦)/main/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.11M/2.11M [00:00<00:00, 32.2MB/s]\n",
      "Downloading (â€¦)in/added_tokens.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80.0/80.0 [00:00<00:00, 40.6kB/s]\n",
      "Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230/230 [00:00<00:00, 126kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"vicgalle/gpt2-open-instruct-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "556d5697-12e6-4fc4-b6c1-ad19fa7508d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vicgalle/gpt2-open-instruct-v1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"vicgalle/gpt2-open-instruct-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e85f73ae-1bcb-4130-9010-f32ff6aa8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0dcbb80-61cc-41a5-91d6-9ff72f00b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce620f36-c1e6-4eaf-9e96-418ab2b9763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_text = \"generate dialogues that imitates Shakespeare's work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8d86aaa-ff4d-4dc6-b05e-7c0c8142f121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghule.k/.conda/envs/llm_env/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "   instruction_text,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85084f29-1a1f-4fb5-a751-0c05defd927c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': \"generate dialogues that imitates Shakespeare's work), the first time I heard of it is when a friend asked me to help him with his homework.\\n\\nYou know, he was trying to finish the first chapter of his novel, but I'm sure he just forgot something important. He's so busy with studying for the next chapter that it's hard to focus and remember things.\\n\\nSo I thought you could ask me something. I think you might be able to do it.\\n\\nOf course, let's do it together. I can make a plan. You could just tell me how much you want to spend on the first chapter, the number of chapters you've already worked on, the amount of time you've been working, and so on. It will be so much fun.\\n\\nBut don't worry about it, buddy. I will make sureÙ‚you get through everything you need to do.\\n\\nI hope you like it. I'm here for\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30ffa0-70a6-4ca0-b5a4-e9c8b4aff2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
