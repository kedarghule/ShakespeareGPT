{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6e7e84-ef28-45a3-b066-1c9c4d3b00f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a98dd38-368b-4c57-b7fd-797a9b655a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad34272-c401-4426-a764-5e3951564485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc3dfe5d0634b3c99ff478a20575b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c69ad3c7314449681d50e772db97c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0c70be99e344f28f47607bc29789dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ef7a72b262439186f9e29b840fffd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558e2573582948cfaef70a5e05e7d5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "config = GPT2Config.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "224a8c20-ed9a-4cf0-957b-4c2ba09dad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "train_path = \"data/input.txt\"  # Path to your training text file\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=train_path,\n",
    "    block_size=128\n",
    ")\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02bb1645-e94a-45ac-9200-8c154a823701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_gpt2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31b9b5cb-a653-40a7-b739-3f988a0a749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshi.at/.conda/envs/ath_pytorch/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='660' max='660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [660/660 03:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.737600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=660, training_loss=3.695005104758523, metrics={'train_runtime': 219.2689, 'train_samples_per_second': 12.04, 'train_steps_per_second': 3.01, 'total_flos': 172452741120000.0, 'train_loss': 3.695005104758523, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "136a8f4c-b4ec-42bb-a633-74bdfb525668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output_gpt2/tokenizer_config.json',\n",
       " 'output_gpt2/special_tokens_map.json',\n",
       " 'output_gpt2/vocab.json',\n",
       " 'output_gpt2/merges.txt',\n",
       " 'output_gpt2/added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "output_path = \"output_gpt2\"\n",
    "model.save_pretrained(output_path)\n",
    "tokenizer.save_pretrained(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e354b0c-f7e2-46cc-803e-b8d9e1e4f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6cec68b-10dd-429d-8f73-d30c2a8d762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"output_gpt2\"  # The directory where the trained model was saved\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(output_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(output_path)\n",
    "\n",
    "# Prepare the input text\n",
    "input_text = \"To be, or not to be\"  # You can use any text you'd like as a starting point\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fea46a9d-7c2e-4e09-939b-5bc2a03789f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_return_sequences has to be 1, but is 10 when doing greedy search.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-12028585c812>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_repeat_ngram_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/ath_pytorch/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ath_pytorch/lib/python3.7/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_return_sequences\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m                 raise ValueError(\n\u001b[0;32m-> 1432\u001b[0;31m                     \u001b[0;34mf\"num_return_sequences has to be 1, but is {generation_config.num_return_sequences} when doing\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1433\u001b[0m                     \u001b[0;34m\" greedy search.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m                 )\n",
      "\u001b[0;31mValueError\u001b[0m: num_return_sequences has to be 1, but is 10 when doing greedy search."
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids, max_length=10, num_return_sequences=1, no_repeat_ngram_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c018080d-3729-40f6-b69e-83ee2cfd3b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in epoch 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(\"in epoch {}\".format(i))\n",
    "    # Generate text using the model\n",
    "    output = model.generate(input_ids, max_length=1024, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    text_file = open(\"gpt2_sample.txt\", \"w\")\n",
    "    n = text_file.write(generated_text)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "274b4df3-fc7d-46b4-a586-a1960eb509bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbd8005a-4a05-4688-b3a3-72f14b8001e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"gpt2_sample.txt\", \"w\")\n",
    "n = text_file.write(generated_text)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd447d89-520c-48ed-bd1c-e51a19571fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To be, or not to be?\\n\\nKING RICHARD II:\\nI have no doubt, sir, that you are.\\nBut, if you were, I would have you to your bed. I\\nwould have your head, your hands, and your heart. You\\nhave been so much in the way of my life,\\nthat I have been unable to bear it. But, as I am, you\\nare not so far from me. What, then, is'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98591565-c602-4e60-828f-a03c5bdb418f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9216"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3c15331-8170-4f8e-9179-4c3b484bad22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f854c32f137f40e893b63eed8e374a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7f3de027f845a185ae673a64722179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d0a991f93f44e1b44bd480873b67a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4642416f27804d509d113335d0313cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4521f272ecca4650a88e9768be5168d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3ac40049644d28896c6ae3e220def7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd953ee0ea949ad8b8d58ccb46ddad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/80.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae6dbde205645dca838c3be56190d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"vicgalle/gpt2-open-instruct-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "556d5697-12e6-4fc4-b6c1-ad19fa7508d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vicgalle/gpt2-open-instruct-v1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"vicgalle/gpt2-open-instruct-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e85f73ae-1bcb-4130-9010-f32ff6aa8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0dcbb80-61cc-41a5-91d6-9ff72f00b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce620f36-c1e6-4eaf-9e96-418ab2b9763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_text = \"generate text that imitates Shakespeare's work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8d86aaa-ff4d-4dc6-b05e-7c0c8142f121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "   instruction_text,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85084f29-1a1f-4fb5-a751-0c05defd927c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': 'generate text that imitates Shakespeare\\'s work.) If you would like to use the same word or phrase in a new text please provide the text as follows:\\n\\n\"A man walks through a valley, and the wind blows fiercely and fiercely, until the sun goes down. A man stands before the river, and a man stands before the sea. A man walks through a forest, and his breath is like a river of blood. The wind beats fiercely, and the sun rises. A man stands before the river, and the river blows fiercely and fiercely. A man stands before the river, and the river blows fiercely and fiercely. A man stands before the river; and the river blows fiercely and fiercely, and the wind blows fiercely.\"\\nA man walks through a valley, and the wind blows fiercely and fiercely, until the sun goes down. A man stands before the river, and a man stands before the sea. A man stands before the river, and the river blows fiercely and fiercely'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30ffa0-70a6-4ca0-b5a4-e9c8b4aff2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ath_pytorch",
   "language": "python",
   "name": "ath_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
